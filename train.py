"""
CartPole-v1 í™˜ê²½ì—ì„œ DQN ì—ì´ì „íŠ¸ í•™ìŠµ
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import torch
import random
from dqn_agent import DQNAgent
from tqdm import tqdm
import platform
import os


def setup_matplotlib():
    """
    Matplotlib ì„¤ì • (í°íŠ¸ ë¬¸ì œ ì™„ì „ í•´ê²°)
    í•œê¸€ í°íŠ¸ ì—†ì´ë„ ì‘ë™í•˜ë„ë¡ ì˜ë¬¸ ì „ìš©ìœ¼ë¡œ ì„¤ì •
    """
    # ê¸°ë³¸ í°íŠ¸ë¥¼ sans-serifë¡œ ì„¤ì • (ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False
    
    # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.rcParams['figure.dpi'] = 100
    plt.rcParams['savefig.dpi'] = 300
    plt.rcParams['figure.autolayout'] = False
    
    print("Matplotlib ì„¤ì • ì™„ë£Œ (ì˜ë¬¸ í°íŠ¸ ì‚¬ìš©)")


def set_seed(seed=42):
    """
    ì¬í˜„ì„±ì„ ìœ„í•œ Random Seed ê³ ì •
    
    Args:
        seed (int): ë‚œìˆ˜ ì‹œë“œ ê°’ (ê¸°ë³¸ê°’: 42)
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def plot_rewards(rewards, save_path='result_graph.png'):
    """
    í•™ìŠµ ê³¼ì •ì˜ ë³´ìƒ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë¦¬ê³  ì €ì¥
    
    Args:
        rewards (list): ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ ë¦¬ìŠ¤íŠ¸
        save_path (str): ê·¸ë˜í”„ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    plt.figure(figsize=(12, 6))
    
    # ì—í”¼ì†Œë“œë³„ ë³´ìƒ ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(rewards, alpha=0.6, label='Episode Reward')
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Total Reward', fontsize=12)
    plt.title('DQN Training Progress - Episode Rewards', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # ì´ë™ í‰ê·  (100 ì—í”¼ì†Œë“œ ë‹¨ìœ„)
    plt.subplot(1, 2, 2)
    if len(rewards) >= 100:
        moving_avg = [np.mean(rewards[max(0, i-99):i+1]) for i in range(len(rewards))]
        plt.plot(moving_avg, color='red', linewidth=2, label='Moving Average (100 episodes)')
        plt.axhline(y=195, color='green', linestyle='--', label='Target Score (195)')
    else:
        plt.plot(rewards, color='red', linewidth=2, label='Total Reward')
    
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Average Reward', fontsize=12)
    plt.title('DQN Training Progress - Moving Average', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nê·¸ë˜í”„ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.close()


def train_dqn(
    n_episodes=1000,
    max_steps=500,
    target_score=195.0,
    print_interval=100,
    model_save_path='cartpole_dqn.pth'
):
    """
    DQN ì—ì´ì „íŠ¸ë¥¼ CartPole-v1 í™˜ê²½ì—ì„œ í•™ìŠµ
    
    Args:
        n_episodes (int): ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 1000)
        max_steps (int): ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 500)
        target_score (float): ëª©í‘œ í‰ê·  ì ìˆ˜ (ê¸°ë³¸ê°’: 195.0)
        print_interval (int): ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²© (ê¸°ë³¸ê°’: 100 ì—í”¼ì†Œë“œ)
        model_save_path (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: 'cartpole_dqn.pth')
    """
    
    # Matplotlib ì„¤ì • (ëª¨ë“  í™˜ê²½ ì§€ì›)
    setup_matplotlib()
    
    # Random Seed ê³ ì • (ì¬í˜„ì„± í™•ë³´)
    set_seed(42)
    
    # CartPole-v1 í™˜ê²½ ìƒì„±
    env = gym.make('CartPole-v1')
    
    # ìƒíƒœ ë° í–‰ë™ ê³µê°„ í¬ê¸° í™•ì¸
    state_size = env.observation_space.shape[0]  # CartPole: 4 (ìœ„ì¹˜, ì†ë„, ê°ë„, ê°ì†ë„)
    action_size = env.action_space.n  # CartPole: 2 (ì™¼ìª½, ì˜¤ë¥¸ìª½)
    
    print("=" * 60)
    print("CartPole-v1 DQN í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print(f"ìƒíƒœ ê³µê°„ í¬ê¸°: {state_size}")
    print(f"í–‰ë™ ê³µê°„ í¬ê¸°: {action_size}")
    print(f"ëª©í‘œ í‰ê·  ì ìˆ˜: {target_score}")
    print(f"ì´ ì—í”¼ì†Œë“œ ìˆ˜: {n_episodes}")
    print("=" * 60)
    
    # DQN ì—ì´ì „íŠ¸ ìƒì„±
    agent = DQNAgent(
        state_size=state_size,
        action_size=action_size,
        learning_rate=0.001,      # í•™ìŠµë¥ 
        gamma=0.99,               # í• ì¸ìœ¨
        epsilon_start=1.0,        # ì´ˆê¸° íƒí—˜ í™•ë¥ 
        epsilon_end=0.01,         # ìµœì†Œ íƒí—˜ í™•ë¥ 
        epsilon_decay=0.995,      # íƒí—˜ í™•ë¥  ê°ì†Œìœ¨
        buffer_size=10000,        # Replay Buffer í¬ê¸°
        batch_size=64,            # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
        target_update_freq=10     # Target Network ì—…ë°ì´íŠ¸ ë¹ˆë„ (ì—í”¼ì†Œë“œ ë‹¨ìœ„)
    )
    
    # í•™ìŠµ ê¸°ë¡ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    episode_rewards = []  # ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ
    recent_scores = []    # ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œì˜ ì ìˆ˜ (ëª©í‘œ ë‹¬ì„± í™•ì¸ìš©)
    
    # í•™ìŠµ ë£¨í”„
    for episode in tqdm(range(1, n_episodes + 1), desc="í•™ìŠµ ì§„í–‰"):
        # í™˜ê²½ ì´ˆê¸°í™”
        state, _ = env.reset(seed=42 + episode)
        total_reward = 0
        
        # ì—í”¼ì†Œë“œ ì‹¤í–‰
        for step in range(max_steps):
            # í–‰ë™ ì„ íƒ (epsilon-greedy)
            action = agent.select_action(state, training=True)
            
            # í™˜ê²½ì—ì„œ í–‰ë™ ì‹¤í–‰
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # ê²½í—˜ì„ Replay Bufferì— ì €ì¥
            agent.store_transition(state, action, reward, next_state, done)
            
            # ì—ì´ì „íŠ¸ í•™ìŠµ
            loss = agent.train()
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = next_state
            total_reward += reward
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ
            if done:
                break
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì²˜ë¦¬
        episode_rewards.append(total_reward)
        recent_scores.append(total_reward)
        if len(recent_scores) > 100:
            recent_scores.pop(0)
        
        # Epsilon ê°ì†Œ (íƒí—˜ í™•ë¥  ì¤„ì´ê¸°)
        agent.update_epsilon()
        
        # Target Network ì—…ë°ì´íŠ¸ (ì¼ì • ì£¼ê¸°ë§ˆë‹¤)
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (100 ì—í”¼ì†Œë“œë§ˆë‹¤)
        if episode % print_interval == 0:
            avg_score = np.mean(recent_scores)
            print(f"\nì—í”¼ì†Œë“œ {episode}/{n_episodes} | "
                  f"í‰ê·  ì ìˆ˜ (ìµœê·¼ 100): {avg_score:.2f} | "
                  f"í˜„ì¬ Epsilon: {agent.epsilon:.3f}")
        
        # ëª©í‘œ ì ìˆ˜ ë‹¬ì„± í™•ì¸ (ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œ í‰ê· )
        if len(recent_scores) >= 100:
            avg_score = np.mean(recent_scores)
            if avg_score >= target_score:
                print("\n" + "=" * 60)
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ì—í”¼ì†Œë“œ {episode}ì—ì„œ í‰ê·  ì ìˆ˜ {avg_score:.2f} ë‹¬ì„±!")
                print("=" * 60)
                agent.save(model_save_path)
                break
    
    # í•™ìŠµ ì¢…ë£Œ ì²˜ë¦¬
    else:
        # ìµœëŒ€ ì—í”¼ì†Œë“œ ë„ë‹¬ ì‹œ ëª¨ë¸ ì €ì¥
        print("\n" + "=" * 60)
        print(f"í•™ìŠµ ì™„ë£Œ! ìµœì¢… í‰ê·  ì ìˆ˜: {np.mean(recent_scores):.2f}")
        print("=" * 60)
        agent.save(model_save_path)
    
    # í™˜ê²½ ì¢…ë£Œ
    env.close()
    
    # í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥
    plot_rewards(episode_rewards)
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("í•™ìŠµ í†µê³„")
    print("=" * 60)
    print(f"ì´ ì—í”¼ì†Œë“œ: {len(episode_rewards)}")
    print(f"ìµœê³  ì ìˆ˜: {max(episode_rewards):.2f}")
    print(f"í‰ê·  ì ìˆ˜: {np.mean(episode_rewards):.2f}")
    print(f"ìµœì¢… 100 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[-100:]):.2f}")
    print("=" * 60)


if __name__ == "__main__":
    # í•™ìŠµ ì‹¤í–‰
    train_dqn(
        n_episodes=1000,           # ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜
        max_steps=500,             # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…
        target_score=195.0,        # ëª©í‘œ í‰ê·  ì ìˆ˜
        print_interval=100,        # ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²©
        model_save_path='cartpole_dqn.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    )
