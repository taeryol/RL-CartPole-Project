"""
CartPole-v1 í™˜ê²½ì—ì„œ DQN ì—ì´ì „íŠ¸ í•™ìŠµ
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import torch
import random
from dqn_agent import DQNAgent
from tqdm import tqdm
import platform
import os


def setup_korean_font():
    """
    í•œê¸€ í°íŠ¸ ì„¤ì • (Google Colab ë° ë‹¤ì–‘í•œ í™˜ê²½ ì§€ì›)
    """
    import matplotlib.font_manager as fm
    
    # ì‹œìŠ¤í…œ ì¢…ë¥˜ í™•ì¸
    system = platform.system()
    
    # Google Colab í™˜ê²½ì¸ ê²½ìš° í°íŠ¸ ì„¤ì¹˜
    try:
        if 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython()):
            print("Google Colab í™˜ê²½ ê°ì§€ - í•œê¸€ í°íŠ¸ ì„¤ì¹˜ ì¤‘...")
            import subprocess
            subprocess.run(['apt-get', 'install', '-y', 'fonts-nanum'], 
                         stdout=subprocess.DEVNULL, 
                         stderr=subprocess.DEVNULL)
            
            # í°íŠ¸ ìºì‹œ ì‚­ì œ ë° ì¬ìƒì„±
            import shutil
            font_cache_dir = os.path.expanduser('~/.cache/matplotlib')
            if os.path.exists(font_cache_dir):
                shutil.rmtree(font_cache_dir)
            
            # ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ì‚¬ìš©
            plt.rcParams['font.family'] = 'NanumGothic'
            print("í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ!")
            return
    except:
        pass
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # ìš°ì„ ìˆœìœ„ëŒ€ë¡œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
    korean_fonts = [
        'NanumGothic',           # Google Colab, Ubuntu
        'Malgun Gothic',         # Windows
        'AppleGothic',           # macOS
        'Noto Sans CJK KR',      # Linux
        'DejaVu Sans'            # ê¸°ë³¸ í°íŠ¸ (í•œê¸€ ë¯¸ì§€ì›)
    ]
    
    selected_font = None
    for font in korean_fonts:
        if font in available_fonts:
            selected_font = font
            break
    
    if selected_font:
        plt.rcParams['font.family'] = selected_font
        print(f"í•œê¸€ í°íŠ¸ ì„¤ì •: {selected_font}")
    else:
        # í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì˜ë¬¸ìœ¼ë¡œ ëŒ€ì²´
        plt.rcParams['font.family'] = 'DejaVu Sans'
        print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False


def set_seed(seed=42):
    """
    ì¬í˜„ì„±ì„ ìœ„í•œ Random Seed ê³ ì •
    
    Args:
        seed (int): ë‚œìˆ˜ ì‹œë“œ ê°’ (ê¸°ë³¸ê°’: 42)
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def plot_rewards(rewards, save_path='result_graph.png'):
    """
    í•™ìŠµ ê³¼ì •ì˜ ë³´ìƒ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë¦¬ê³  ì €ì¥
    
    Args:
        rewards (list): ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ ë¦¬ìŠ¤íŠ¸
        save_path (str): ê·¸ë˜í”„ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    plt.figure(figsize=(12, 6))
    
    # ì—í”¼ì†Œë“œë³„ ë³´ìƒ ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(rewards, alpha=0.6, label='Episode Reward')
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Total Reward', fontsize=12)
    plt.title('DQN Training Progress - Episode Rewards', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # ì´ë™ í‰ê·  (100 ì—í”¼ì†Œë“œ ë‹¨ìœ„)
    plt.subplot(1, 2, 2)
    if len(rewards) >= 100:
        moving_avg = [np.mean(rewards[max(0, i-99):i+1]) for i in range(len(rewards))]
        plt.plot(moving_avg, color='red', linewidth=2, label='Moving Average (100 episodes)')
        plt.axhline(y=195, color='green', linestyle='--', label='Target Score (195)')
    else:
        plt.plot(rewards, color='red', linewidth=2, label='Total Reward')
    
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Average Reward', fontsize=12)
    plt.title('DQN Training Progress - Moving Average', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nê·¸ë˜í”„ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.close()


def train_dqn(
    n_episodes=1000,
    max_steps=500,
    target_score=195.0,
    print_interval=100,
    model_save_path='cartpole_dqn.pth'
):
    """
    DQN ì—ì´ì „íŠ¸ë¥¼ CartPole-v1 í™˜ê²½ì—ì„œ í•™ìŠµ
    
    Args:
        n_episodes (int): ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 1000)
        max_steps (int): ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 500)
        target_score (float): ëª©í‘œ í‰ê·  ì ìˆ˜ (ê¸°ë³¸ê°’: 195.0)
        print_interval (int): ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²© (ê¸°ë³¸ê°’: 100 ì—í”¼ì†Œë“œ)
        model_save_path (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: 'cartpole_dqn.pth')
    """
    
    # í•œê¸€ í°íŠ¸ ì„¤ì • (Google Colab ì§€ì›)
    setup_korean_font()
    
    # Random Seed ê³ ì • (ì¬í˜„ì„± í™•ë³´)
    set_seed(42)
    
    # CartPole-v1 í™˜ê²½ ìƒì„±
    env = gym.make('CartPole-v1')
    
    # ìƒíƒœ ë° í–‰ë™ ê³µê°„ í¬ê¸° í™•ì¸
    state_size = env.observation_space.shape[0]  # CartPole: 4 (ìœ„ì¹˜, ì†ë„, ê°ë„, ê°ì†ë„)
    action_size = env.action_space.n  # CartPole: 2 (ì™¼ìª½, ì˜¤ë¥¸ìª½)
    
    print("=" * 60)
    print("CartPole-v1 DQN í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print(f"ìƒíƒœ ê³µê°„ í¬ê¸°: {state_size}")
    print(f"í–‰ë™ ê³µê°„ í¬ê¸°: {action_size}")
    print(f"ëª©í‘œ í‰ê·  ì ìˆ˜: {target_score}")
    print(f"ì´ ì—í”¼ì†Œë“œ ìˆ˜: {n_episodes}")
    print("=" * 60)
    
    # DQN ì—ì´ì „íŠ¸ ìƒì„±
    agent = DQNAgent(
        state_size=state_size,
        action_size=action_size,
        learning_rate=0.001,      # í•™ìŠµë¥ 
        gamma=0.99,               # í• ì¸ìœ¨
        epsilon_start=1.0,        # ì´ˆê¸° íƒí—˜ í™•ë¥ 
        epsilon_end=0.01,         # ìµœì†Œ íƒí—˜ í™•ë¥ 
        epsilon_decay=0.995,      # íƒí—˜ í™•ë¥  ê°ì†Œìœ¨
        buffer_size=10000,        # Replay Buffer í¬ê¸°
        batch_size=64,            # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
        target_update_freq=10     # Target Network ì—…ë°ì´íŠ¸ ë¹ˆë„ (ì—í”¼ì†Œë“œ ë‹¨ìœ„)
    )
    
    # í•™ìŠµ ê¸°ë¡ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    episode_rewards = []  # ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ
    recent_scores = []    # ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œì˜ ì ìˆ˜ (ëª©í‘œ ë‹¬ì„± í™•ì¸ìš©)
    
    # í•™ìŠµ ë£¨í”„
    for episode in tqdm(range(1, n_episodes + 1), desc="í•™ìŠµ ì§„í–‰"):
        # í™˜ê²½ ì´ˆê¸°í™”
        state, _ = env.reset(seed=42 + episode)
        total_reward = 0
        
        # ì—í”¼ì†Œë“œ ì‹¤í–‰
        for step in range(max_steps):
            # í–‰ë™ ì„ íƒ (epsilon-greedy)
            action = agent.select_action(state, training=True)
            
            # í™˜ê²½ì—ì„œ í–‰ë™ ì‹¤í–‰
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # ê²½í—˜ì„ Replay Bufferì— ì €ì¥
            agent.store_transition(state, action, reward, next_state, done)
            
            # ì—ì´ì „íŠ¸ í•™ìŠµ
            loss = agent.train()
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = next_state
            total_reward += reward
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ
            if done:
                break
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì²˜ë¦¬
        episode_rewards.append(total_reward)
        recent_scores.append(total_reward)
        if len(recent_scores) > 100:
            recent_scores.pop(0)
        
        # Epsilon ê°ì†Œ (íƒí—˜ í™•ë¥  ì¤„ì´ê¸°)
        agent.update_epsilon()
        
        # Target Network ì—…ë°ì´íŠ¸ (ì¼ì • ì£¼ê¸°ë§ˆë‹¤)
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (100 ì—í”¼ì†Œë“œë§ˆë‹¤)
        if episode % print_interval == 0:
            avg_score = np.mean(recent_scores)
            print(f"\nì—í”¼ì†Œë“œ {episode}/{n_episodes} | "
                  f"í‰ê·  ì ìˆ˜ (ìµœê·¼ 100): {avg_score:.2f} | "
                  f"í˜„ì¬ Epsilon: {agent.epsilon:.3f}")
        
        # ëª©í‘œ ì ìˆ˜ ë‹¬ì„± í™•ì¸ (ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œ í‰ê· )
        if len(recent_scores) >= 100:
            avg_score = np.mean(recent_scores)
            if avg_score >= target_score:
                print("\n" + "=" * 60)
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ì—í”¼ì†Œë“œ {episode}ì—ì„œ í‰ê·  ì ìˆ˜ {avg_score:.2f} ë‹¬ì„±!")
                print("=" * 60)
                agent.save(model_save_path)
                break
    
    # í•™ìŠµ ì¢…ë£Œ ì²˜ë¦¬
    else:
        # ìµœëŒ€ ì—í”¼ì†Œë“œ ë„ë‹¬ ì‹œ ëª¨ë¸ ì €ì¥
        print("\n" + "=" * 60)
        print(f"í•™ìŠµ ì™„ë£Œ! ìµœì¢… í‰ê·  ì ìˆ˜: {np.mean(recent_scores):.2f}")
        print("=" * 60)
        agent.save(model_save_path)
    
    # í™˜ê²½ ì¢…ë£Œ
    env.close()
    
    # í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥
    plot_rewards(episode_rewards)
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("í•™ìŠµ í†µê³„")
    print("=" * 60)
    print(f"ì´ ì—í”¼ì†Œë“œ: {len(episode_rewards)}")
    print(f"ìµœê³  ì ìˆ˜: {max(episode_rewards):.2f}")
    print(f"í‰ê·  ì ìˆ˜: {np.mean(episode_rewards):.2f}")
    print(f"ìµœì¢… 100 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[-100:]):.2f}")
    print("=" * 60)


if __name__ == "__main__":
    # í•™ìŠµ ì‹¤í–‰
    train_dqn(
        n_episodes=1000,           # ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜
        max_steps=500,             # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…
        target_score=195.0,        # ëª©í‘œ í‰ê·  ì ìˆ˜
        print_interval=100,        # ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²©
        model_save_path='cartpole_dqn.pth'  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    )
